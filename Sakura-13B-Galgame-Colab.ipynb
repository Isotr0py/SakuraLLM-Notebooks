{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Isotr0py/SakuraLLM-Notebooks/blob/main/Sakura-13B-Galgame-Colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tvkI52m5DRsL"
      },
      "outputs": [],
      "source": [
        "#@title 初始化环境\n",
        "#@markdown 挂载Google网盘\n",
        "Mount_GDrive = False # @param {type:\"boolean\"}\n",
        "if Mount_GDrive:\n",
        "  from google.colab import drive\n",
        "\n",
        "  drive.mount('/content/gdrive')\n",
        "  ROOT_PATH = \"/content/gdrive/MyDrive\"\n",
        "else:\n",
        "  ROOT_PATH = \"/content\"\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gelzXVWEGxZw"
      },
      "outputs": [],
      "source": [
        "#@title 安装依赖\n",
        "%cd $ROOT_PATH\n",
        "!git clone https://github.com/SakuraLLM/Sakura-13B-Galgame.git\n",
        "\n",
        "%cd Sakura-13B-Galgame\n",
        "LLAMA_CPP = True # @param {type:\"boolean\"}\n",
        "VLLM = True # @param {type:\"boolean\"}\n",
        "if LLAMA_CPP:\n",
        "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
        "if VLLM:\n",
        "  !pip install -q -U torch --index-url https://download.pytorch.org/whl/cu121\n",
        "  !pip install vllm\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install -q pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DUQnJQ96Jau8"
      },
      "outputs": [],
      "source": [
        "#@title 翻译EPUB\n",
        "MODEL = \"SakuraLLM/Sakura-13B-LNovel-v0_8-4bit\" # @param [\"SakuraLLM/Sakura-13B-LNovel-v0_8-3bit\", \"SakuraLLM/Sakura-13B-LNovel-v0_8-4bit\", \"SakuraLLM/Sakura-13B-LNovel-v0_8-8bit\"]\n",
        "EPUB_PATH = \"novel.epub\" # @param {type:\"string\"}\n",
        "OUTPUT_FOLDER = \"output/\" # @param {type:\"string\"}\n",
        "\n",
        "%cd $ROOT_PATH/Sakura-13B-Galgame\n",
        "!python translate_epub.py \\\n",
        "    --model_name_or_path $MODEL \\\n",
        "    --trust_remote_code \\\n",
        "    --model_version 0.8 \\\n",
        "    --use_gptq_model \\\n",
        "    --text_length 512 \\\n",
        "    --data_path $EPUB_PATH \\\n",
        "    --output_folder $OUTPUT_FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S9gYbA1yMVND"
      },
      "outputs": [],
      "source": [
        "#@title 翻译文本文件\n",
        "MODEL = \"SakuraLLM/Sakura-13B-LNovel-v0_8-4bit\" # @param [\"SakuraLLM/Sakura-13B-LNovel-v0_8-3bit\", \"SakuraLLM/Sakura-13B-LNovel-v0_8-4bit\", \"SakuraLLM/Sakura-13B-LNovel-v0_8-8bit\"]\n",
        "DATA_PATH = \"novel.txt\" # @param {type:\"string\"}\n",
        "OUTPUT_PATH = \"novel_translated.txt\" # @param {type:\"string\"}\n",
        "\n",
        "%cd $ROOT_PATH/Sakura-13B-Galgame\n",
        "!python translate_novel.py \\\n",
        "    --model_name_or_path $MODEL \\\n",
        "    --trust_remote_code \\\n",
        "    --model_version 0.8 \\\n",
        "    --use_gptq_model \\\n",
        "    --text_length 512 \\\n",
        "    --data_path $DATA_PATH \\\n",
        "    --output_path $OUTPUT_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wo7uQVKUABIY"
      },
      "outputs": [],
      "source": [
        "#@title 运行API后端(Transformers AutoGPTQ)\n",
        "#@markdown 使用[ngrok](https://ngrok.com/)进行API映射\n",
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "ngrokToken = \"\"  # @param {type:\"string\"}\n",
        "if ngrokToken:\n",
        "  from pyngrok import conf, ngrok\n",
        "  conf.get_default().auth_token = ngrokToken\n",
        "  conf.get_default().monitor_thread = False\n",
        "  ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n",
        "  if len(ssh_tunnels) == 0:\n",
        "      ssh_tunnel = ngrok.connect(5000)\n",
        "      print('address：'+ssh_tunnel.public_url)\n",
        "  else:\n",
        "      print('address：'+ssh_tunnels[0].public_url)\n",
        "else:\n",
        "  import subprocess\n",
        "  import threading\n",
        "  def start_localtunnel(port):\n",
        "      p = subprocess.Popen([\"npx\", \"localtunnel\", \"--port\", f\"{port}\"], stdout=subprocess.PIPE)\n",
        "      for line in p.stdout:\n",
        "          print(line.decode(), end='')\n",
        "  threading.Thread(target=start_localtunnel, daemon=True, args=(5000,)).start()\n",
        "\n",
        "\n",
        "%cd $ROOT_PATH/Sakura-13B-Galgame\n",
        "MODEL = \"SakuraLLM/Sakura-13B-LNovel-v0_8-4bit\" # @param [\"SakuraLLM/Sakura-13B-LNovel-v0_8-3bit\", \"SakuraLLM/Sakura-13B-LNovel-v0_8-4bit\", \"SakuraLLM/Sakura-13B-LNovel-v0_8-8bit\"]\n",
        "MODEL_PATH = f\"models/{MODEL.split('/')[-1]}\"\n",
        "if not Path(MODEL_PATH).exists():\n",
        "  snapshot_download(repo_id=MODEL,local_dir=MODEL_PATH,local_dir_use_symlinks=False)\n",
        "  !sed -i '521c\\        return nn.functional.linear(hidden_states, norm_weight.to(hidden_states.dtype))' $MODEL_PATH/modeling_baichuan.py\n",
        "\n",
        "!python server.py \\\n",
        "  --model_name_or_path $MODEL_PATH \\\n",
        "  --use_gptq_model \\\n",
        "  --model_version 0.8 \\\n",
        "  --trust_remote_code \\\n",
        "  --no-auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 运行API后端(vLLM)\n",
        "#@markdown 使用[ngrok](https://ngrok.com/)进行API映射（ngrokToken留空则使用[localtunnel](https://github.com/localtunnel/localtunnel)）\n",
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "ngrokToken = \"\"  # @param {type:\"string\"}\n",
        "if ngrokToken:\n",
        "  from pyngrok import conf, ngrok\n",
        "  conf.get_default().auth_token = ngrokToken\n",
        "  conf.get_default().monitor_thread = False\n",
        "  ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n",
        "  if len(ssh_tunnels) == 0:\n",
        "      ssh_tunnel = ngrok.connect(5000)\n",
        "      print('address：'+ssh_tunnel.public_url)\n",
        "  else:\n",
        "      print('address：'+ssh_tunnels[0].public_url)\n",
        "else:\n",
        "  import subprocess\n",
        "  import threading\n",
        "  def start_localtunnel(port):\n",
        "      p = subprocess.Popen([\"npx\", \"localtunnel\", \"--port\", f\"{port}\"], stdout=subprocess.PIPE)\n",
        "      for line in p.stdout:\n",
        "          print(line.decode(), end='')\n",
        "  threading.Thread(target=start_localtunnel, daemon=True, args=(5000,)).start()\n",
        "\n",
        "\n",
        "%cd $ROOT_PATH/Sakura-13B-Galgame\n",
        "MODEL = \"SakuraLLM/Sakura-13B-LNovel-v0_8-4bit\" # @param [\"SakuraLLM/Sakura-13B-LNovel-v0_8-4bit\"]\n",
        "\n",
        "!RAY_memory_monitor_refresh_ms=\"0\" python server.py \\\n",
        "  --model_name_or_path $MODEL \\\n",
        "  --vllm \\\n",
        "  --use_gptq_model \\\n",
        "  --model_version 0.8 \\\n",
        "  --trust_remote_code \\\n",
        "  --no-auth \\\n",
        "  --enforce_eager \\\n",
        "  --gpu_memory_utilization 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 运行API后端(llama-cpp-python)\n",
        "#@markdown 使用[ngrok](https://ngrok.com/)进行API映射（ngrokToken留空则使用[localtunnel](https://github.com/localtunnel/localtunnel)）\n",
        "from huggingface_hub import hf_hub_download\n",
        "from pathlib import Path\n",
        "\n",
        "ngrokToken = \"\"  # @param {type:\"string\"}\n",
        "if ngrokToken:\n",
        "  from pyngrok import conf, ngrok\n",
        "  conf.get_default().auth_token = ngrokToken\n",
        "  conf.get_default().monitor_thread = False\n",
        "  ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n",
        "  if len(ssh_tunnels) == 0:\n",
        "      ssh_tunnel = ngrok.connect(5000)\n",
        "      print('address：'+ssh_tunnel.public_url)\n",
        "  else:\n",
        "      print('address：'+ssh_tunnels[0].public_url)\n",
        "else:\n",
        "  import subprocess\n",
        "  import threading\n",
        "  def start_localtunnel(port):\n",
        "      p = subprocess.Popen([\"npx\", \"localtunnel\", \"--port\", f\"{port}\"], stdout=subprocess.PIPE)\n",
        "      for line in p.stdout:\n",
        "          print(line.decode(), end='')\n",
        "  threading.Thread(target=start_localtunnel, daemon=True, args=(5000,)).start()\n",
        "\n",
        "\n",
        "%cd $ROOT_PATH/Sakura-13B-Galgame\n",
        "MODEL = \"sakura-13b-lnovel-v0.9-Q4KM_awq4bit\" # @param [\"sakura-13b-lnovel-v0.9-Q8_0\", \"sakura-13b-lnovel-v0.9-Q6_K\", \"sakura-13b-lnovel-v0.9-Q5_K_M\", \"sakura-13b-lnovel-v0.9-Q4KM_awq4bit\", \"sakura-13b-lnovel-v0.9-Q4_K_M\", \"sakura-13b-lnovel-v0.9-Q3_K_M\", \"sakura-13b-lnovel-v0.9-Q2_K\"]\n",
        "MODEL_PATH = f\"./models/{MODEL}.gguf\"\n",
        "if not Path(MODEL_PATH).exists():\n",
        "    hf_hub_download(repo_id=\"SakuraLLM/Sakura-13B-LNovel-v0.9b-GGUF\", filename=f\"{MODEL}.gguf\", local_dir=\"models/\")\n",
        "\n",
        "!python server.py \\\n",
        "  --model_name_or_path $MODEL_PATH \\\n",
        "  --llama_cpp \\\n",
        "  --use_gpu \\\n",
        "  --model_version 0.9 \\\n",
        "  --trust_remote_code \\\n",
        "  --no-auth"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
